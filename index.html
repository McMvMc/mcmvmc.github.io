<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shengze Wang</title>
  
  <meta name="author" content="Shengze Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shengze Wang</name>
              </p>
              <p>
                I joined NVIDIA Research in 2025 as a postdoctoral researcher, working at the intersection of 
                Computer Vision, Graphics, and AI with a focus on modeling human geometry and behaviors for 
                lifelike robots. 
                My past research spans 3D reconstruction and rendering, human pose estimation, generative models, 
                SLAM, and telepresence systems. 
              </p>
              <p>
                During my Ph.D. study on telepresence systems at the University of North Carolina at Chapel Hill, I was fortunate to have been advised by 
                professor <a href="https://henryfuchs.web.unc.edu/">Henry Fuchs</a>.  
                Before that, I worked with professor <a href="https://dhoiem.cs.illinois.edu/"> Derek Hoiem </a> at his startup <a href="https://reconstructinc.com/">Reconstruct</a>.                
                I obtained my Master of Science in Computer Vision (MSCV) from Carnegie Mellon University, where I worked with professor <a href="http://www.cs.cmu.edu/~kaess/">Michael Kaess</a> on SLAM systems.
                I obtained my Bachelor of Science in Computer Engineering from University of Illinois at Urbana Champaign.
              </p>
              <p style="text-align:center">
                shengzew [at] cs.unc.edu</a> &nbsp/&nbsp
<!--                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp-->
<!--                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.com/citations?user=R7GGKKAAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/mct1224">Twitter</a> &nbsp/&nbsp
                <a href="https://soundcloud.com/levme/tracks">My Music (Soundcloud)</a> &nbsp/&nbsp
                <a href="https://music.163.com/#/artist?id=35510973">ÁΩëÊòì‰∫ë</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/mike.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/mike.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

            <!-- NEW: BLADE (CVPR 2025) -->
            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
                <img src='images/blade.png' alt="BLADE teaser" style="margin-top:0px; width:100%">
              </td>
              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_BLADE_Single-view_Body_Mesh_Estimation_through_Accurate_Depth_Estimation_CVPR_2025_paper.pdf">
                  <papertitle>BLADE: Single-view Body Mesh Estimation through Accurate Depth Estimation</papertitle>
                </a>
                <br>
                <b>Shengze Wang</b>, <a href="https://jeffli.site/">Jiefeng Li</a>, 
                <a href="https://tianyeli.github.io/"></a>Tianye Li</a>, 
                <a href="https://ye-yuan.com/"></a>Ye Yuan</a>, 
                <a href="https://henryfuchs.web.unc.edu/">Henry Fuchs</a>, 
                <a href="https://luminohope.org/">Koki Nagano</a>*, 
                <a href="https://research.nvidia.com/person/shalini-de-mello">Shalini De Mello</a>*, 
                <a href="https://research.nvidia.com/person/michael-stengel">Michael Stengel</a>* 
                <br>
                <em>CVPR</em>, 2025
                &nbsp; [<a href="https://research.nvidia.com/labs/amri/projects/blade/">Project Page</a>]
                &nbsp; [<a href="https://cvpr.thecvf.com/virtual/2025/poster/32659">CVPR</a>]
                &nbsp; [<a href="">Code</a>]
                &nbsp; [<a href="blade_weights.html">Weights & File Preparation</a>]
                <p>
                  Single-image human mesh recovery that accurately estimates perspective parameters (including <i>T<sub>z</sub></i> and focal length) from a single image, delivering SOTA pose accuracy and 2D alignment‚Äîespecially for close-range views.
                </p>
              </td>
            </tr>

            <!-- NEW: Coherent3D (CVPR 2025) -->
            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
                <img src='images/coherent3d.png' alt="Coherent3D teaser" style="margin-top:0px; width:70%;margin-left:15%;">
              </td>
              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://arxiv.org/abs/2412.08684">
                  <papertitle>Coherent3D: Coherent 3D Portrait Video Reconstruction via Triplane Fusion</papertitle>
                </a>
                <br>
                <b>Shengze Wang</b>,  
                <a href="https://sunshineatnoon.github.io/">Xueting Li</a>, 
                <a href="https://research.nvidia.com/person/chao-liu">Chao Liu</a>, 
                <a href="https://matthew-a-chan.github.io/">Matthew Chan</a>, 
                <a href="https://research.nvidia.com/person/michael-stengel">Michael Stengel</a>,
                <a href="https://henryfuchs.web.unc.edu/">Henry Fuchs</a>, 
                <a href="https://research.nvidia.com/person/shalini-de-mello">Shalini De Mello</a>*, 
                <a href="https://luminohope.org/">Koki Nagano</a>*, 
                <br>
                <em>CVPR</em>, 2025
                &nbsp; [<a href="https://research.nvidia.com/labs/amri/projects/coherent3d">Project Page</a>]
                &nbsp; [<a href="https://cvpr.thecvf.com/virtual/2025/poster/33109">CVPR</a>]
                <p>
                  Fusion-based 3D portrait method that combines a canonical 3D prior with per-frame appearance to achieve temporally stable, identity-consistent 3D videos for single-camera telepresence.
                </p>
              </td>
            </tr>

            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
                <img src='images/mnaf.png' alt="MNAF teaser" style="margin-top:0px; width:100%">
              </td>
              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://www.cs.unc.edu/~cpk/data/papers/Neural_acoustic_fields.pdf">
                  <papertitle>Multimodal Neural Acoustic Fields for Immersive Mixed Reality</papertitle> <font color="red">(IEEE VRBest Paper Honorable Mention Award)</font>
                </a>
                <br>
                Guansen Tong, Johnathan Chi-Ho Leung, Xi Peng, Haosheng Shi, Liujie Zheng, <b>Shengze Wang</b>, Arryn Carlos O‚ÄôBrien, Ashley Paula-Ann Neall, Grace Fei, Martim Gaspar, <a href="https://www.cs.unc.edu/~cpk/">Praneeth Chakravarthula</a>
                <br>
                <em>IEEE TVCG</em>, 2025
                &nbsp; [<a href="https://dl.acm.org/doi/abs/10.1109/TVCG.2025.3549898">TVCG</a>]
                <p>
                  Visual-acoustic fusion (‚Äúconformer‚Äù) + acoustic synthesis to learn scene acoustics and render spatial audio from novel viewpoints; improves immersive MR audio quality.
                </p>
              </td>
            </tr>

            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
                <img src='images/my3dgen.png' alt="My3DGen teaser" style="margin-top:0px; width:100%">
              </td>
              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Qi_My3DGen_A_Scalable_Personalized_3D_Generative_Model_WACV_2025_paper.pdf">
                  <papertitle>My3DGen: A Scalable Personalized 3D Generative Model</papertitle>
                </a>
                <br>
                Luchao Qi, Jiaye Wu, Annie N. Wang, <b>Shengze Wang</b>, <a href="https://www.cs.unc.edu/~ronisen/">Roni Sengupta</a>
                <br>
                <em>WACV</em>, 2025
                &nbsp; [<a href="https://my3dgen.github.io/">Project Page</a>]
                &nbsp; [<a href="https://arxiv.org/abs/2307.05468">arXiv</a>]
                <p>
                  Personalized 3D prior of an individual using as few as 50 training images. My3DGen allows for novel view synthesis, semantic editing of a given face (e.g. adding a smile), and synthesizing novel appearances, all while preserving the original person's identity.
                </p>
              </td>
            </tr>

            <!-- NEW: TVCG 2025 journal extension of desktop telepresence -->
            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
                <video preload='auto' autoplay muted loop playsinline style="margin-top:0px; width:100%">
                  <source src="./videos/test9.mp4" type="video/mp4">
                </video>
              </td>
              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://www.computer.org/csdl/journal/tg/2025/09/10557509/1XLFWwMYZgc">
                  <papertitle>Learning View Synthesis for Desktop Telepresence With Few RGBD Cameras</papertitle>
                </a>
                <br>
                <b>Shengze Wang</b>, Ziheng Wang, Ryan Schmelzle, Liuejie Zheng,
                <a href="https://youngjoongunc.github.io/">YoungJoong Kwon</a>,
                <a href="https://www.cs.unc.edu/~ronisen/">Roni Sengupta</a>,
                <a href="https://henryfuchs.web.unc.edu/">Henry Fuchs</a>
                <br>
                <em>IEEE TVCG</em>, 2025
                &nbsp; [<a href="https://mcmvmc.github.io/PersonalTelepresence/">Project Page</a>]
                &nbsp; [<a href="https://arxiv.org/abs/2304.01197">arXiv</a>]
                <p>
                  Journal version of your affordable desktop telepresence system (Bringing Telepresence to Every Desk, 2023): 4 consumer RGB-D cameras and a renderer that synthesizes high-quality free-viewpoint videos of both user and environment.
                </p>
              </td>
            </tr>


            <!-- NEW: AI-Mediated 3D Video Conferencing (SIGGRAPH E-Tech 2023) -->
            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
                <img src='images/3dvc.png' alt="AI-Mediated 3DVC teaser" style="margin-top:0px; width:100%">
              </td>
              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://research.nvidia.com/publication/2023-08_ai-mediated-3d-video-conferencing">
                  <papertitle>AI-Mediated 3D Video Conferencing</papertitle>
                </a>
                <br>
                <a href="https://research.nvidia.com/person/michael-stengel">Michael Stengel</a>, 
                <a href="https://luminohope.org/">Koki Nagano</a>, 
                <a href="https://research.nvidia.com/person/chao-liu">Chao Liu</a>, 
                <a href="https://matthew-a-chan.github.io/">Matthew Chan</a>, 
                <a href="https://alextrevithick.com/">Alex Trevithick</a>, 
                <a href="https://research.nvidia.com/person/shalini-de-mello">Shalini De Mello</a>, 
                <a href="https://research.nvidia.com/person/jonghyun-kim">Jonghyun Kim</a>, 
                <a href="https://luebke.us/">David Luebke</a>, 
                <a href="http://amritamaz.net/">Amrita Mazumdar</a>, 
                <b>Shengze Wang</b>
                <a href="https://www.linkedin.com/in/mayoorejaiswal/">Mayoore Jaiswal</a>
                <br>
                <em>SIGGRAPH Emerging Technologies</em>, 2023
                &nbsp; [<a href="https://research.nvidia.com/labs/nxp/3dvc-siggraph-etech/">Project Page</a>]
                &nbsp; [<a href="https://www.youtube.com/watch?reload=9&v=sZefJHL8X_4">NVIDIA YouTube</a>]
                &nbsp; [<a href="https://dl.acm.org/doi/10.1145/3588037.3595385">ACM DL</a>]
                <p>
                  We present an AI-mediated 3D video conferencing system that can reconstruct and autostereoscopically display a life-sized talking head using consumer-grade compute resources and minimal capture equipment. Our 3D capture uses a novel 3D lifting method that encodes a given 2D input into an efficient triplanar neural representation of the user, which can be rendered from novel viewpoints in real-time. Our AI-based techniques drastically reduce the cost for 3D capture, while providing a high-fidelity 3D representation on the receiver's end at the cost of traditional 2D video streaming. Additional advantages of our AI-based approach include the ability to accommodate both photorealistic and stylized avatars, and the ability to enable mutual eye contact in multi-directional video conferencing. We demonstrate our system using a tracked stereo display for a personal viewing experience as well as a light field display for a multi-viewer experience.
                </p>
              </td>
            </tr>

            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
                  <video preload='auto' autoplay muted loop playsinline style="margin-top:0px; width:100%">
                    <source src="./videos/test9.mp4"
                            type="video/mp4">
                  </video>
              </td>

              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="">
                  <papertitle> Bringing Telepresence to Every Desk </papertitle>
                </a>
                <br>
                <b>Shengze Wang</b>, Ziheng Wang, Ryan Schmelzle, Liuejie Zheng,
                <a href="https://youngjoongunc.github.io/">YoungJoong Kwon</a>,
                <a href="https://www.cs.unc.edu/~ronisen/">Roni Sengupta</a>,
                <a href="https://henryfuchs.web.unc.edu/">Henry Fuchs</a>
                <br>
                <em>arXiv</em>, 2023    [<a href="https://mcmvmc.github.io/PersonalTelepresence/">Project Page</a>][Code]
                <p>
                    We showcase a prototype personal telepresence system. With 4 RGBD cameras, it can be easily
                    installed on every desk. Our renderer synthesizes high-quality free-viewpoint videos of the
                    entire scene and outperforms prior neural rendering methods.
                </p>
              </td>
            </tr>


            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
<!--                <div class="one">-->
<!--                  <div class="two"></div>-->
                  <img src='images/INV.png' style="margin-top:0px; width:100%">
<!--                </div>-->
              </td>

              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://arxiv.org/abs/2302.01532">
                  <papertitle> INV: Towards Streaming Incremental Neural Videos </papertitle>
                </a>
                <br>
                <b>Shengze Wang</b>, Alexey Supikov, Joshua Ratcliff,
                <a href="https://henryfuchs.web.unc.edu/">Henry Fuchs</a>,
                <a href="https://ronaldazuma.com/">Ronald Azuma</a>
                <br>
                <em>arXiv</em>, 2023
                <p>
                    We discovered a <b> natural information partition </b> in 2D/3D MLPs, which stores structural
                    information in early layers and color information in later layers.
                    We leverage this property to incrementally stream dynamic free-viewpoint videos <b> without buffering </b>
                    (required by prior dynamic NeRFs).
                </p><p>
                    With the significant reduction in training time and bandwidth, we lay foundation for live-streaming
                    NeRF and better understanding of MLPs.
                </p>
              </td>
            </tr>


            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
<!--                <div class="one">-->
<!--                  <div class="two"></div>-->
                  <img src='images/PLC-LiSLAM.png' style="margin-top:0px; width:100%">
<!--                </div>-->
              </td>

              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://ieeexplore.ieee.org/abstract/document/9787712">
                  <papertitle> PLC-LiSLAM: LiDAR SLAM With Planes, Lines, and Cylinders </papertitle>
                </a>
                <br>
                Lipu Zhou, Guoquan Huang, Yinian Mao, Jincheng Yu, <b>Shengze Wang</b>,
                <a href="http://www.cs.cmu.edu/~kaess/">Michael Kaess</a>
                <br>
                <em>IEEE RA-L</em>, 2022
              </td>
            </tr>


            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
<!--                <div class="one">-->
<!--                  <div class="two"></div>-->
                  <img src='images/EDPLVO.png' style="margin-top:0px; width:100%">
<!--                </div>-->
              </td>

              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://ieeexplore.ieee.org/abstract/document/9812133">
                  <papertitle> EDPLVO: Efficient direct point-line visual odometry </papertitle>
                    <font color="red">(Outstanding Navigation Paper)</font>
                </a>
                <br>
                Lipu Zhou, Guoquan Huang, Yinian Mao, <b>Shengze Wang</b>,
                <a href="http://www.cs.cmu.edu/~kaess/">Michael Kaess</a>
                <br>
                <em>ICRA</em>, 2022
              </td>
            </tr>


            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
<!--                <div class="one">-->
<!--                  <div class="two"></div>-->
                  <img src='images/DFVS.png' style="margin-top:0px; width:100%">
<!--                </div>-->
              </td>

              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://arxiv.org/abs/2204.10477v1">
                  <papertitle> Learning Dynamic View Synthesis With Few RGBD Cameras </papertitle>
                </a>
                <br>
                <b>Shengze Wang</b>,
                <a href="https://youngjoongunc.github.io/">YoungJoong Kwon</a>,
                Yuan Shen, Qian Zhang, Andrei State,
                  <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>,
                <a href="https://henryfuchs.web.unc.edu/">Henry Fuchs</a>
                <br>
                <em>arXiv</em>, 2022
                <p>
                    We introduce a system that synthesizes dynamic free-viewpoint videos from 2 RGBD cameras. This
                    is a preliminary work to our personal telepresence system.
                </p>
              </td>
            </tr>


            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
<!--                <div class="one">-->
<!--                  <div class="two"></div>-->
                  <img src='images/dplvo.png' style="margin-top:0px; width:100%">
<!--                </div>-->
              </td>

              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://ieeexplore.ieee.org/abstract/document/9484792">
                  <papertitle> DPLVO: direct point-line monocular visual odometry </papertitle>
                </a>
                <br>
                Lipu Zhou, <b>Shengze Wang</b>,
                <a href="http://www.cs.cmu.edu/~kaess/">Michael Kaess</a>
                <br>
                <em>ICRA</em>, 2021
              </td>
            </tr>


            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
<!--                <div class="one">-->
<!--                  <div class="two"></div>-->
                  <img src='images/pi_slam.png' style="margin-top:0px; width:100%">
<!--                </div>-->
              </td>

              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://ieeexplore.ieee.org/abstract/document/9561933">
                  <papertitle> œÄ-LSAM: LiDAR Smoothing and Mapping With Planes </papertitle>
                </a>
                <br>
                Lipu Zhou, <b>Shengze Wang</b>,
                <a href="http://www.cs.cmu.edu/~kaess/">Michael Kaess</a>
                <br>
                <em>ICRA</em>, 2021
              </td>
            </tr>


            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
                <div class="one">
                  <div class="two"></div>
      <!--            <img src='images/pi_slam.png' width="160" style="margin-top:40px">-->
                </div>
              </td>

              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://ieeexplore.ieee.org/document/9197023">
                  <papertitle> A fast and accurate solution for pose estimation from 3D correspondences </papertitle>
                </a>
                <br>
                Lipu Zhou, <b>Shengze Wang</b>,
                <a href="http://www.cs.cmu.edu/~kaess/">Michael Kaess</a>
                <br>
                <em>ICRA</em>, 2020
              </td>
            </tr>


            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
<!--                <div class="one">-->
      <!--            <div class="two"><img src='images/mono_depth.png'></div>-->
                  <img src='images/do_not_omit.png' style="margin-top:0px; width:100%">
<!--                </div>-->
              </td>

              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://arxiv.org/abs/1904.01759">
                  <papertitle> Do not Omit Local Minimizer: a Complete Solution for Pose Estimation from 3D Correspondences </papertitle>
                </a>
                <br>
                Lipu Zhou, <b>Shengze Wang</b>, Jiamin Ye,
                <a href="http://www.cs.cmu.edu/~kaess/">Michael Kaess</a>
                <br>
                <em>arXiv</em>, 2019
              </td>
            </tr>


            <tr bgcolor="#ffffff">
              <td style="padding:10px;width:35%;vertical-align:middle">
<!--                <div class="one">-->
      <!--            <div class="two"><img src='images/mono_depth.png'></div>-->
                  <img src='images/mono_depth.png' style="margin-top:0px; width:100%">
<!--                </div>-->
              </td>

              <td style="padding-left:10px; padding-top:30px; width:65%;vertical-align:top;">
                <a href="https://arxiv.org/abs/1812.03368">
                  <papertitle> Unsupervised Learning of Monocular Depth Estimation with Bundle Adjustment, Super-Resolution and Clip Loss </papertitle>
                </a>
                <br>
                Lipu Zhou, Jiamin Ye, Montiel Abello, <b>Shengze Wang</b>,
                <a href="http://www.cs.cmu.edu/~kaess/">Michael Kaess</a>
                <br>
                <em>arXiv</em>, 2018
              </td>
            </tr>




					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Thank you to Jon Barron for sharing his <a href="https://github.com/jonbarron/jonbarron_website">website template</a> with the community

              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
